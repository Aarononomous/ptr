{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Tree Readability\n",
    "\n",
    "This Jupyter notebook is for developing, explaining, and experimenting with parse tree readability, a measure of readability I'm developing based on the premises that word length and sentence length are not the most important qualities of a piece of text when it comes to determining the difficulty of understanding it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Imports\n",
    "\n",
    "We'll need several libraries for the corpora, for simple text analysis, and for parsing sentences.\n",
    "\n",
    "In addition, we'll set up this notebook's graphing displays and create global variables for global things, such as American English, `en_nlp`; and a grammar derived from the Penn Treebank, which, when we need it, will be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "from math import sqrt, log\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk import Tree, Production, Nonterminal, CFG, ChartParser\n",
    "import textacy\n",
    "import pyphen  # hyphenation library\n",
    "import spacy\n",
    "\n",
    "pyphen.language_fallback('en_US')\n",
    "dic = pyphen.Pyphen(lang='en_US')\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "productions = None  # to be created as needed, later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we'll do is pull in our corpus. The corpus needs to contain a variety of reading levels within it.\n",
    "\n",
    "Additionally, we need to have an already-parse corpus. We can use the Penn Treebank for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Parse Tree\n",
    "\n",
    "Sometimes we'll need to get back the original sentence from a parse tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_sentence(parse_tree):\n",
    "    \"\"\"\n",
    "    Get back the original text of a sentence.\n",
    "    \"\"\"\n",
    "    leading_tags = set(['(', '$', '``'])  # don't need a space after, but do before\n",
    "    following_tags = set([')', ',', '.', ':', \"''\", 'POS'])  # need a space after, but not before\n",
    "    sentence = ''\n",
    "    no_sp = False\n",
    "    for w, pos in parse_tree.pos():\n",
    "        if pos != '-NONE-':\n",
    "            if pos in following_tags or w in {'%'}:\n",
    "                sentence += w\n",
    "            else:\n",
    "                sentence += ('' if no_sp else ' ') + w\n",
    "            no_sp = pos in leading_tags\n",
    "    return sentence[1:]  # ignore leading space\n",
    "\n",
    "def recreate_sentences(parse_trees):\n",
    "    return [recreate_sentence(parse_tree) for parse_tree in parse_trees]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj = nltk.corpus.treebank.parsed_sents('wsj_0012.mrg')\n",
    "w = wsj[0]\n",
    "recreate_sentences(wsj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production = w.productions()[0]\n",
    "print(production)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I'm able to, I'd like to use tagged, but non-parsed sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: get some small texts which are tagged with Penn Treebank tags\n",
    "\n",
    "\n",
    "So, what are the reading levels of these per the current gold standard SMOG?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: get the SMOG scores of each one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap of the SMOG scores.\n",
    "\n",
    "[[ TODO: if I'm going to not use individual works, but per-genre works, I'll need to be politic about leaping to conclusions. ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's begin to create our parsing functions. These are going to start simply, and get more complicated.\n",
    "\n",
    "## Textual metrics\n",
    "\n",
    "Let's begin with text-based metrics: sentence length and syllabification. We'll use the syllabification library `pyphen` to count the syllables in each word.\n",
    "\n",
    "We can build from these to create the standard text-based metrics, such as SMOG, Flesch-Kincaid, etc.\n",
    "\n",
    "Also note that every one of these takes a POS-tagged sentence, such as in the Treebank corpus, arranged as a list of (word/punctuation, POS) tuples. They do not take text, so it's important, if you want to use these on text, to tag the text beforehand. This can be done with, e.g., spaCy:\n",
    "\n",
    "```python\n",
    "doc = en_nlp(u'One morning, when Gregor Samsa woke from troubled dreams, \\\n",
    "he found himself transformed in his bed into a horrible vermin. He lay on \\\n",
    "his armour-like back, and if he lifted his head a little he could see his \\\n",
    "brown belly, slightly domed and divided by arches into stiff sections.')\n",
    "[[(word.text, word.tag_) for word in sent] for sent in doc.sents]\n",
    "# returns [[('One', 'CD'), ('morning', 'NN'), (',', ','), ('when', 'WRB'), ...], ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def depunctuate(tagged_sentence):\n",
    "    \"\"\"\n",
    "    From a tagged sentence (as a list), returns a sentence without punctuation\n",
    "    \"\"\"\n",
    "    punct_tags = set(['(', ')', ',', '--', '.', ':', '-NONE-', '``', \"''\", '$'])\n",
    "    return ((word, tag) for word, tag in tagged_sentence\n",
    "            if tag not in punct_tags)\n",
    "    \n",
    "def n_words(tagged_sentence):\n",
    "    \"\"\"\n",
    "    From a tagged sentence (as a list), returns the number of words in it,\n",
    "    sans punctuation.\n",
    "    \"\"\"\n",
    "    return sum(1 for word in depunctuate(tagged_sentence))\n",
    "\n",
    "def word_lengths(tagged_sentence):\n",
    "    \"\"\"\n",
    "    From a tagged sentence (as a list), returns a list of non-punctation\n",
    "    word lengths\n",
    "    \"\"\"\n",
    "    return [len(word) for word, pair in depunctuate(tagged_sentence)]\n",
    "\n",
    "def avg_word_length(tagged_sentence):\n",
    "    \"\"\"The average word length of the sentence\"\"\"\n",
    "    lengths = word_lengths(tagged_sentence)\n",
    "    return sum(lengths) / len(lengths)\n",
    "\n",
    "def syllables(tagged_sentence):\n",
    "    \"\"\"\n",
    "    From a tagged sentence (as a list), returns a list of non-punctation\n",
    "    syllables per word.\n",
    "    \"\"\"\n",
    "    return (len(dic.positions(word)) + 1\n",
    "            for word, pair in depunctuate(tagged_sentence))\n",
    "\n",
    "def n_monosyllable_words(tagged_sentence):\n",
    "    \"\"\"\n",
    "    Returns the number of one syllable words in the (word, tag)-list sentence\n",
    "    \"\"\"\n",
    "    return len([sylls for sylls in syllables(tagged_sentence) if sylls == 1])\n",
    "\n",
    "def n_polysyllable_words(tagged_sentence):\n",
    "    \"\"\"\n",
    "    Returns the number of 3+ syllable words in the (word, tag)-list sentence\n",
    "    \"\"\"\n",
    "    return len([sylls for sylls in syllables(tagged_sentence) if sylls >= 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing these functions on a few sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w0, w1, w2 = wsj[:3]\n",
    "print(w1.leaves(), '\\n')\n",
    "print(list(syllables(w1.pos())))\n",
    "print(n_polysyllable_words(w0.pos()))\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Readability Metrics\n",
    "\n",
    "With these building blocks in place, we can create our own versions of the standard readability metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SMOG(text):\n",
    "    \"\"\"\n",
    "    Computes the SMOG score of a piece of text, in this case, a list of tagged sentences.\n",
    "    There should be at least 30 sentences in the text.\n",
    "    \n",
    "    McLaughlin, G. Harry (May 1969). \"SMOG Grading — a New Readability Formula\" (PDF).\n",
    "    Journal of Reading. 12 (8): 639–646.\n",
    "    \"\"\"\n",
    "    n_polysyllables = sum(n_polysyllable_words(sentence) for sentence in text)\n",
    "    n_sentences = len(text)\n",
    "    grade = 1.0430 * sqrt(n_polysyllables * (30 / n_sentences)) + 3.1291\n",
    "    return grade\n",
    "    \n",
    "def flesch_kincaid_grade_level(text):\n",
    "    \"\"\"\n",
    "    Computes the Flesch-Kincaid grade level of a piece of text, in this case,\n",
    "    a list of tagged sentences.\n",
    "\n",
    "    Kincaid JP, Fishburne RP Jr, Rogers RL, Chissom BS (February 1975).\n",
    "    \"Derivation of new readability formulas (Automated Readability Index,\n",
    "    Fog Count and Flesch Reading Ease Formula) for Navy enlisted personnel\".\n",
    "    Research Branch Report 8-75, Millington, TN: Naval Technical Training,\n",
    "    U. S. Naval Air Station, Memphis, TN.\n",
    "    \"\"\"\n",
    "    n_sentences = len(text)\n",
    "    n_words_ = sum(n_words(sentence) for sentence in text)\n",
    "    n_syllables = sum(sum(syllables(sentence)) for sentence in text)\n",
    "    grade = 0.39 * (n_words_ / n_sentences) + 11.8 * (n_syllables/n_words_) - 15.59\n",
    "    return grade\n",
    "\n",
    "def dale_chall(text):\n",
    "    \"\"\"\n",
    "    Computes the Dale-Chall Formula for readability, 1961 revision, for a list\n",
    "    of tagged sentences\n",
    "    \n",
    "    McCallum, D. and Peterson, J. (1982), Computer-Based Readability Indexes,\n",
    "    Proceedings of the ACM '82 Conference, (October 1982), 44-48. \n",
    "    \"\"\"\n",
    "    f = open('dale-chall.txt')\n",
    "    dale_long_list = [word.lower() for word in f.read().split()]\n",
    "    f.close()\n",
    "    \n",
    "    n_dale_long = sum(1 for sentence in text for word, POS in sentence\n",
    "                      if word.lower() in dale_long_list)\n",
    "    n_words_ = sum(n_words(sentence) for sentence in text)\n",
    "    n_sentences = len(text)\n",
    "    grade = 14.863 - 11.42 * (n_dale_long / n_words_) + 0.0512 * (n_words_ / n_sentences)\n",
    "    return grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextStats:\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        # The corpus is a tagged sentence or list of tagged sentences\n",
    "        self.corpus = [corpus] if isinstance(corpus[0], tuple) else corpus\n",
    "\n",
    "    def basic_counts(self):\n",
    "        n_s = len(self.corpus)\n",
    "        return {\n",
    "            'n_sentences': n_s,\n",
    "            'n_words': sum(n_words(s) for s in self.corpus),\n",
    "            'n_chars': sum(sum(word_lengths(s)) for s in self.corpus),\n",
    "            'syllables': sum(sum(syllables(s)) for s in self.corpus),\n",
    "            'n_monosyllable_words': sum(n_monosyllable_words(s)\n",
    "                                        for s in self.corpus),\n",
    "            'n_polysyllable_words': sum(n_polysyllable_words(s)\n",
    "                                        for s in self.corpus),\n",
    "        }\n",
    "\n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            'SMOG': SMOG(self.corpus),\n",
    "            'flesch_kincaid_grade_level': flesch_kincaid_grade_level(self.corpus),\n",
    "            'dale_chall': dale_chall(self.corpus),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def depunctuate(tagged_sentence):\n",
    "        \"\"\"\n",
    "        From a tagged sentence (as a list), returns a sentence without punctuation\n",
    "        \"\"\"\n",
    "        punct_tags = set(['(', ')', ',', '--', '.', ':',\n",
    "                          '-NONE-', '``', \"''\", '$'])\n",
    "        return ((word, tag) for word, tag in tagged_sentence\n",
    "                if tag not in punct_tags)\n",
    "\n",
    "    @staticmethod\n",
    "    def n_words(tagged_sentence):\n",
    "        \"\"\"\n",
    "        From a tagged sentence (as a list), returns the number of words in it,\n",
    "        sans punctuation.\n",
    "        \"\"\"\n",
    "        return sum(1 for word in depunctuate(tagged_sentence))\n",
    "\n",
    "    @staticmethod\n",
    "    def word_lengths(tagged_sentence):\n",
    "        \"\"\"\n",
    "        From a tagged sentence (as a list), returns a list of non-punctation\n",
    "        word lengths\n",
    "        \"\"\"\n",
    "        return [len(word) for word, pair in depunctuate(tagged_sentence)]\n",
    "\n",
    "    @staticmethod\n",
    "    def avg_word_length(tagged_sentence):\n",
    "        \"\"\"The average word length of the sentence\"\"\"\n",
    "        lengths = word_lengths(tagged_sentence)\n",
    "        return sum(lengths) / len(lengths)\n",
    "\n",
    "    @staticmethod\n",
    "    def syllables(tagged_sentence):\n",
    "        \"\"\"\n",
    "        From a tagged sentence (as a list), returns a list of non-punctation\n",
    "        syllables per word.\n",
    "        \"\"\"\n",
    "        return (len(dic.positions(word)) + 1\n",
    "                for word, pair in depunctuate(tagged_sentence))\n",
    "\n",
    "    @staticmethod\n",
    "    def n_monosyllable_words(tagged_sentence):\n",
    "        \"\"\"\n",
    "        Returns the number of one syllable words in the (word, tag)-list sentence\n",
    "        \"\"\"\n",
    "        return len([sylls for sylls in syllables(tagged_sentence) if sylls == 1])\n",
    "\n",
    "    @staticmethod\n",
    "    def n_polysyllable_words(tagged_sentence):\n",
    "        \"\"\"\n",
    "        Returns the number of 3+ syllable words in the (word, tag)-list sentence\n",
    "        \"\"\"\n",
    "        return len([sylls for sylls in syllables(tagged_sentence) if sylls >= 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def SMOG(text):\n",
    "        \"\"\"\n",
    "        Computes the SMOG score of a piece of text, in this case, a list of\n",
    "        tagged sentences. There should be at least 30 sentences in the text.\n",
    "\n",
    "        McLaughlin, G. Harry (May 1969). \"SMOG Grading — a New Readability\n",
    "        Formula\" (PDF). Journal of Reading. 12 (8): 639–646.\n",
    "        \"\"\"\n",
    "        n_polysyllables = sum(n_polysyllable_words(sentence)\n",
    "                              for sentence in text)\n",
    "        n_sentences = len(text)\n",
    "        grade = 1.0430 * sqrt(n_polysyllables * (30 / n_sentences)) + 3.1291\n",
    "        return grade\n",
    "\n",
    "    @staticmethod\n",
    "    def flesch_kincaid_grade_level(text):\n",
    "        \"\"\"\n",
    "        Computes the Flesch-Kincaid grade level of a piece of text, in this case,\n",
    "        a list of tagged sentences.\n",
    "\n",
    "        Kincaid JP, Fishburne RP Jr, Rogers RL, Chissom BS (February 1975).\n",
    "        \"Derivation of new readability formulas (Automated Readability Index,\n",
    "        Fog Count and Flesch Reading Ease Formula) for Navy enlisted personnel\".\n",
    "        Research Branch Report 8-75, Millington, TN: Naval Technical Training,\n",
    "        U. S. Naval Air Station, Memphis, TN.\n",
    "        \"\"\"\n",
    "        n_sentences = len(text)\n",
    "        n_words_ = sum(n_words(sentence) for sentence in text)\n",
    "        n_syllables = sum(sum(syllables(sentence)) for sentence in text)\n",
    "        grade = 0.39 * (n_words_ / n_sentences) + 11.8 * \\\n",
    "            (n_syllables/n_words_) - 15.59\n",
    "        return grade\n",
    "\n",
    "    @staticmethod\n",
    "    def dale_chall(text):\n",
    "        \"\"\"\n",
    "        Computes the Dale-Chall Formula for readability, 1961 revision, for a\n",
    "        list of tagged sentences.\n",
    "\n",
    "        McCallum, D. and Peterson, J. (1982), Computer-Based Readability\n",
    "        Indexes, Proceedings of the ACM '82 Conference, (October 1982), 44-48.\n",
    "        \"\"\"\n",
    "        f = open('dale-chall.txt')\n",
    "        dale_long_list = [word.lower() for word in f.read().split()]\n",
    "        f.close()\n",
    "\n",
    "        n_dale_long = sum(1 for sentence in text for word, POS in sentence\n",
    "                          if word.lower() in dale_long_list)\n",
    "        n_words_ = sum(n_words(sentence) for sentence in text)\n",
    "        n_sentences = len(text)\n",
    "        grade = 14.863 - 11.42 * (n_dale_long / n_words_) + \\\n",
    "            0.0512 * (n_words_ / n_sentences)\n",
    "        return grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj_pos = [s.pos() for s in wsj]\n",
    "TextStats(wsj_pos).get_stats()\n",
    "TextStats(wsj_pos).basic_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare this to the output of textacy -- it's close!\n",
    "wsj_pos = [s.pos() for s in wsj]\n",
    "text = '\\n'.join(recreate_sentences(wsj))\n",
    "doc = textacy.Doc(text)\n",
    "\n",
    "# t_words = list(textacy.text_stats.extract.words(doc, filter_punct=True, filter_stops=False, filter_nums=False))\n",
    "# my_words = (word for sentence in wsj_pos for word, pos in depunctuate(sentence))\n",
    "# print(list(zip(t_words, my_words)))\n",
    "# print('*' * 80)\n",
    "\n",
    "ts = textacy.text_stats.TextStats(doc)\n",
    "print(ts.basic_counts)\n",
    "print('n_sents:', len(wsj_pos))\n",
    "print('n_words:', sum(n_words(sentence) for sentence in wsj_pos))\n",
    "print('n_syllables', sum(sum(syllables(sentence)) for sentence in wsj_pos))\n",
    "print('n_monosyllable_words', sum(n_monosyllable_words(sentence) for sentence in wsj_pos))\n",
    "print('n_polysyllable_words', sum(n_polysyllable_words(sentence) for sentence in wsj_pos))\n",
    "\n",
    "print('*' * 80)\n",
    "print('SMOG:', SMOG(wsj_pos))\n",
    "print('FK:', flesch_kincaid_grade_level(wsj_pos))\n",
    "print('Dale-Chall:', dale_chall(wsj_pos))\n",
    "print(ts.readability_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next round of text analysis functions, let's dig a little deeper into the POS analysis. Remember that this is still sans parse-tree-ification.\n",
    "\n",
    "## Tag-based Metrics\n",
    "\n",
    "These use the POS tags. Since we're using the Penn Treebank, we need to make sure that whatever tags we're using are within this tagset; some tagged texts from the Brown Corpus, for example, use other tagsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_sentence(sentence):\n",
    "    \"\"\"A helper method to easily tokenize a sentence\"\"\"\n",
    "    return nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "\n",
    "def tag_sentences(sentences):\n",
    "    \"\"\"A helper method to easily tokenize sentences\"\"\"\n",
    "    return [tag_sentence(sent) for sent in nltk.sent_tokenize(sentences)]\n",
    "\n",
    "\n",
    "class TagStats:\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        def n_repeated_possessives(tagged_sentence):\n",
    "            \"\"\"\n",
    "            The number of 2+ repeated possessives, e.g.:\n",
    "            \"John's mother's neighbor's uncle's dog's Instagram account\" -> 5\n",
    "            \"John's dog's Instagram account\" -> 2\n",
    "            \"John's Instagram account\" -> 0  # because it's not repeated\n",
    "            \"\"\"\n",
    "            max_count = 0\n",
    "\n",
    "            count = 0\n",
    "            loc = -1\n",
    "            for i, (word, pos) in enumerate(tagged_sentence[::2]):\n",
    "                if pos == 'POS':\n",
    "                    count += 1\n",
    "                    max_count = max(max_count, count)\n",
    "                    if loc != i - 1:  # it's not repeated\n",
    "                        count = 0\n",
    "                    loc = i\n",
    "\n",
    "            count = 0\n",
    "            loc = -1\n",
    "            for i, (word, pos) in enumerate(tagged_sentence[1::2]):\n",
    "                if pos == 'POS':\n",
    "                    count += 1\n",
    "                    max_count = max(max_count, count)\n",
    "                    if loc != i - 1:  # it's not repeated\n",
    "                        count = 0\n",
    "                    loc = i\n",
    "\n",
    "            return max_count + 1 if max_count >= 1 else 0\n",
    "\n",
    "        def n_repeated_adverbs(tagged_sentence):\n",
    "            \"\"\"\n",
    "            The number of 2+ repeated adverbs (RB, RBR, and RBS/RBT) e.g.:\n",
    "            \"harder better faster stronger\" -> 4  # as adverbs, not adjectives!\n",
    "            \"most happily\" -> 2\n",
    "            \"likely ready\" -> 0  # because \"ready\" is an adjective\n",
    "            \"\"\"\n",
    "            adverbs = {'RB', 'RBR', 'RBS', 'RBT', 'RB$',\n",
    "                       'RB+BEZ', 'RB+CS', 'RBR+CS', 'RN', 'RP'}\n",
    "            max_count = 0\n",
    "\n",
    "            count = 0\n",
    "            loc = -1\n",
    "            for i, (word, pos) in enumerate(tagged_sentence):\n",
    "                if pos in adverbs:\n",
    "                    count += 1\n",
    "                    max_count = max(max_count, count)\n",
    "                    if loc != i - 1:  # it's not repeated\n",
    "                        count = 0\n",
    "                    loc = i\n",
    "\n",
    "            return max_count if max_count >= 2 else 0\n",
    "\n",
    "        # the corpus is a tagged sentence or list of tagged sentences\n",
    "        self.corpus = [corpus] if isinstance(corpus[0], tuple) else corpus\n",
    "\n",
    "        self.POSs = [set(pos for word, pos in sentence) for sentence in corpus]\n",
    "        self.n_pronouns = [len([pos for word, pos in sentence if pos == 'PRP'])\n",
    "                           for sentence in corpus]\n",
    "        self.n_repeated_possessives = [n_repeated_possessives(sentence)\n",
    "                                       for sentence in corpus]\n",
    "        self.n_repeated_adverbs = [n_repeated_adverbs(sentence)\n",
    "                                   for sentence in corpus]\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Combine all the statistics together\"\"\"\n",
    "        ...\n",
    "        n_s = len(self.corpus)\n",
    "        t_pos = len({POS for POSs in self.POSs for POS in POSs})\n",
    "        a_pos = sum(len(POSs) for POSs in self.POSs) / n_s\n",
    "        a_r_p = sum(x for x in self.n_repeated_possessives) / n_s\n",
    "        a_r_a = sum(x for x in self.n_repeated_adverbs) / n_s\n",
    "\n",
    "        return {\n",
    "            'total_POSs': t_pos,\n",
    "            'avg_POSs': a_pos,\n",
    "            'avg_pronouns': a_pro,\n",
    "            'avg_repeated_possessives': a_r_p,\n",
    "            'avg_repeated_adverbs': a_r_a,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = nltk.PerceptronTagger()\n",
    "model = pt.__dict__['model']\n",
    "w = model.__dict__['weights']\n",
    "f = open('tags.txt', 'w')\n",
    "for x in w.items():\n",
    "    print(x, file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = wsj[7].pos()\n",
    "print(w)\n",
    "print('POS types:', n_POSs(w))\n",
    "print('n_pronouns:', n_pronouns(w))\n",
    "r_p_sent = [('My', 'PRP$'), ('',''), ('dog', 'NN'), (\"'s\", 'POS'), ('therapist', 'NN'), (\"'s\", 'POS'),\n",
    "            ('uncle', 'NN'), (\"'s\", 'POS'), ('friend', 'NN'), (\"'s\", 'VBZ'), ('smiling', 'VBG')]\n",
    "\n",
    "r_a_sent = [('My', 'PRP$'), ('very', 'RB'), ('very', 'RB'), ('very', 'RB'), ('very', 'RB'), ('good', 'JJ'),\n",
    "            ('dog', 'NN')]\n",
    "\n",
    "print('n_rep_poss:', n_repeated_possessives(r_p_sent))  # should be 3, not 4\n",
    "print('n_rep_adv:', n_repeated_adverbs(r_a_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based Metrics\n",
    "\n",
    "These use the structure of the parse trees, not the productions (yet!) to measure things. We'll start with simple measurements, and build on top of those.\n",
    "\n",
    "First, of course, we need to get the parse trees. [ TODO ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_productions():\n",
    "    \"\"\"\n",
    "    We'll amortize the cost of initializing the grammar, done by reading\n",
    "    through the Penn Treebank, by only doing it once.\n",
    "    Note that this creates only non-terminal productions. All of the\n",
    "    terminal productions need to be created as well, then appended to the\n",
    "    tb_grammar before creating a parser. Use create_terminals for this.\n",
    "    \"\"\"\n",
    "    global productions\n",
    "    global branch_factor  # the average length of the rhs\n",
    "    \n",
    "    def filter_prod(production):\n",
    "        \"\"\"Removes dashed subtypes in nonterminal productions\"\"\"\n",
    "        def filter_subtypes(nonterminal):\n",
    "            if nonterminal.symbol() == '-NONE-':\n",
    "                return nonterminal\n",
    "            else:\n",
    "                return Nonterminal(nonterminal.symbol().split('-')[0])\n",
    "\n",
    "        l = filter_subtypes(production.lhs())\n",
    "        r = tuple(filter_subtypes(p) for p in production.rhs())\n",
    "        return Production(l, r)\n",
    "\n",
    "    if productions is None:\n",
    "        treebank = nltk.corpus.treebank.parsed_sents()\n",
    "        tb_productions = [production for t in treebank for production in t.productions()\n",
    "                          if isinstance(production.rhs()[0], Nonterminal)]\n",
    "        # simplify the productions\n",
    "        simplified_productions = [filter_prod(p) for p in tb_productions]\n",
    "        counter = Counter(simplified_productions)\n",
    "        # 7,982 nonterminal rules -- we need to reduce this greatly\n",
    "        top_prods = {p for p, c in counter.most_common(250)}  # magic number\n",
    "        productions = top_prods\n",
    "        branch_factor = sum(len(p.rhs()) for p in productions) / len(productions)\n",
    "\n",
    "def create_terminals(tagged_text):\n",
    "    \"\"\"\n",
    "    We need to add terminals to our grammar as well. This takes a tagged\n",
    "    sentence, e.g. [('The', 'DT'), ...] and returns a list of terminal\n",
    "    productions.\n",
    "    The tagged text may also be a list of tagged sentences, in which case\n",
    "    we add all words from these sentences to terminal productions\n",
    "    \"\"\"\n",
    "    if isinstance(tagged_text[0], tuple):\n",
    "        return set(Production(Nonterminal(tag), (word,))\n",
    "                   for word, tag in tagged_text)\n",
    "    else:\n",
    "        return set(Production(Nonterminal(tag), (word,))\n",
    "                   for tagged_sentence in tagged_text\n",
    "                   for word, tag in tagged_sentence)\n",
    "\n",
    "def create_parser(tagged_text):\n",
    "    \"\"\"\n",
    "    Return a parser for the tagged text, which, as in `create_terminals`,\n",
    "    can be either a POS-tagged sentence or a list of them.\n",
    "    \"\"\"\n",
    "    global productions\n",
    "    init_productions()\n",
    "    # add tagged words to the parser\n",
    "    productions |= create_terminals(tagged_text)\n",
    "    grammar = CFG(start=Nonterminal('S'), productions=productions)\n",
    "    return nltk.ChartParser(grammar)\n",
    "\n",
    "def possible_parses(tagged_sentence):\n",
    "    \"\"\"\n",
    "    Similar to the above, but sees how many of these possible productions\n",
    "    were actually considered by the parser.\n",
    "    \"\"\"\n",
    "    parser = create_parser(tagged_sentence)\n",
    "    chart = parser.chart_parse((word for word, pos in sent))\n",
    "    return len(chart.edges())\n",
    "\n",
    "def get_trees(tagged_sentence):\n",
    "    \"\"\"\n",
    "    Returns all possible parse trees from a sentence.\n",
    "    The sentence is POS-tagged, and this returns a list of Trees.\n",
    "    \"\"\"\n",
    "    parser = create_parser(tagged_sentence)\n",
    "    parsing = parser.parse(word for word, pos in tagged_sentence)\n",
    "    return [tree for tree in parsing]\n",
    "\n",
    "def n_trees(trees):\n",
    "    if trees:\n",
    "        return len(trees)\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def tree_depth(tree):\n",
    "    \"\"\"Returns the tree depth of an individual parse tree. O(n)\"\"\"\n",
    "    return tree.height()\n",
    "\n",
    "def max_tree_depth(trees, corpus=[]):\n",
    "    \"\"\"\n",
    "    Returns the max tree depth over a collection of parse trees.\n",
    "    If there are no trees (if there are no parses, e.g.), an optional\n",
    "    argument of the original corpus is used to provide an estimate.\n",
    "    \"\"\"\n",
    "    if trees:\n",
    "        return max((tree_depth(tree) for tree in trees))\n",
    "    elif corpus:\n",
    "        # Return the average tree height based on branching on the rhs of \n",
    "        # productions. I.e., if we have the rule X -> Y Z W, there are three branches,\n",
    "        # and so on. Averaging over all the production rules will give us a\n",
    "        # branching factor of, say, 2.6. The average tree height is then \n",
    "        # log_bf(tree length) + 5. (5 is a fudge factor)\n",
    "        ests = (math.ceil(log(len(sentence)/log(branch_factor)) + 5)\n",
    "               for sentence in corpus)\n",
    "        return max(ests)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def avg_tree_depth(trees, corpus=[]):\n",
    "    \"\"\"\n",
    "    Returns the max tree depth over a collection of parse trees.\n",
    "    If there are no trees (if there are no parses, e.g.), an optional\n",
    "    argument of the original corpus is used to provide an estimate.\n",
    "    \"\"\"\n",
    "    if trees:\n",
    "        return sum((tree_depth(tree) for tree in trees))/ len(trees)\n",
    "    elif corpus:\n",
    "        # Return the average tree height based on branching on the rhs of \n",
    "        # productions. I.e., if we have the rule X -> Y Z W, there are three branches,\n",
    "        # and so on. Averaging over all the production rules will give us a\n",
    "        # branching factor of, say, 2.6. The average tree height is then \n",
    "        # log_bf(tree length) + 2. (2 is the POS and terminal string)\n",
    "        ests = (log(len(sentence)/log(branch_factor)) + 5\n",
    "                for sentence in corpus)\n",
    "        return sum(ests) / len(corpus)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def combined(corpus):\n",
    "    \"\"\"Avoids extra work. Use this for faster computation.\"\"\"\n",
    "    trees = get_trees(corpus)\n",
    "    m_t_d = max_tree_depth(trees, corpus=corpus)\n",
    "    a_t_d = avg_tree_depth(trees, corpus=corpus)\n",
    "    return {\n",
    "        'n_trees': n_trees(trees),\n",
    "        'max_tree_depth': m_t_d,\n",
    "        'avg_tree_depth': a_t_d\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeStats:\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"\n",
    "        We'll amortize the cost of initializing the grammar, done by reading\n",
    "        through the Penn Treebank, by only doing it once.\n",
    "        Note that this creates only non-terminal productions. All of the\n",
    "        terminal productions need to be created as well, then appended to the\n",
    "        tb_grammar before creating a parser. Use create_terminals for this.\n",
    "        \"\"\"\n",
    "\n",
    "        def filter_prod(production):\n",
    "            \"\"\"Removes dashed subtypes in nonterminal productions\"\"\"\n",
    "            def filter_subtypes(nonterminal):\n",
    "                if nonterminal.symbol() == '-NONE-':\n",
    "                    return nonterminal\n",
    "                else:\n",
    "                    return Nonterminal(nonterminal.symbol().split('-')[0])\n",
    "\n",
    "            l = filter_subtypes(production.lhs())\n",
    "            r = tuple(filter_subtypes(p) for p in production.rhs())\n",
    "            return Production(l, r)\n",
    "\n",
    "        def create_terminals(tagged_text):\n",
    "            \"\"\"\n",
    "            We need to add terminals to our grammar as well. This takes a tagged\n",
    "            sentence, e.g. [('The', 'DT'), ...] and returns a list of terminal\n",
    "            productions.\n",
    "            The tagged text may also be a list of tagged sentences, in which case\n",
    "            we add all words from these sentences to terminal productions\n",
    "            \"\"\"\n",
    "            if isinstance(tagged_text[0], tuple):\n",
    "                return set(Production(Nonterminal(tag), (word,))\n",
    "                           for word, tag in tagged_text)\n",
    "            else:\n",
    "                return set(Production(Nonterminal(tag), (word,))\n",
    "                           for tagged_sentence in tagged_text\n",
    "                           for word, tag in tagged_sentence)\n",
    "\n",
    "        def get_trees(tagged_sentence):\n",
    "            \"\"\"\n",
    "            Returns all possible parse trees from a single sentence (presumably in the corpus).\n",
    "            The sentence is POS-tagged, and this returns a list of Trees.\n",
    "            \"\"\"\n",
    "            parsing = self.parser.parse(word for word, pos in tagged_sentence)\n",
    "            return [tree for tree in parsing]\n",
    "\n",
    "        def n_productions(parse_tree, production):\n",
    "            \"\"\"Returns the number of productions of type `production` in the parse_tree\"\"\"\n",
    "            productions = list(parse_tree.subtrees(filter=lambda t: t.label() == production))\n",
    "            return len(productions) \n",
    "\n",
    "        self.corpus = corpus\n",
    "        self.productions = None\n",
    "        self.branch_factor = None  # the average length of the rhs\n",
    "        self.grammar = None\n",
    "        self.parser = None\n",
    "\n",
    "        treebank = nltk.corpus.treebank.parsed_sents()\n",
    "        tb_productions = [production\n",
    "                          for tree in treebank\n",
    "                          for production in tree.productions()\n",
    "                          if isinstance(production.rhs()[0], Nonterminal)]\n",
    "\n",
    "        # simplify the productions\n",
    "        simplified_productions = [filter_prod(p) for p in tb_productions]\n",
    "        counter = Counter(simplified_productions)\n",
    "        top_prods = {p for p, c in counter.most_common(250)}  # magic number\n",
    "        self.productions = top_prods\n",
    "        branch_factor = sum(len(p.rhs())\n",
    "                            for p in self.productions) / len(self.productions)\n",
    "\n",
    "        self.productions |= create_terminals(self.corpus)\n",
    "        self.grammar = CFG(start=Nonterminal(\n",
    "            'S'), productions=self.productions)\n",
    "        self.parser = nltk.ChartParser(self.grammar)\n",
    "        all_parse_trees = (get_trees(sentence) for sentence in corpus)\n",
    "\n",
    "        # create statistics sentence by sentence\n",
    "        self.stats = []\n",
    "        for i, sentence_parse_trees in enumerate(all_parse_trees):\n",
    "            print(i)\n",
    "            sent_stats = {}\n",
    "            if sentence_parse_trees:\n",
    "                n_trees = len(sentence_parse_trees)\n",
    "                avg_tree_depth = sum(tree.height()\n",
    "                                     for tree in sentence_parse_trees) / n_trees\n",
    "                max_tree_depth = max(tree.height()\n",
    "                                     for tree in sentence_parse_trees)\n",
    "                avg_noun_phrases = sum(n_productions(tree, 'NP')\n",
    "                                       for tree in sentence_parse_trees) / n_trees\n",
    "                avg_prep_phrases = sum(n_productions(tree, 'PP')\n",
    "                                       for tree in sentence_parse_trees) / n_trees\n",
    "                avg_sbars = sum(n_productions(tree, 'SBAR')\n",
    "                                       for tree in sentence_parse_trees) / n_trees\n",
    "                avg_nonterminals = sum(len(tree.productions())\n",
    "                                       for tree in sentence_parse_trees) / n_trees\n",
    "            else:\n",
    "                n_trees = 1  # presumably there's at least one.\n",
    "                avg_tree_depth = log(len(corpus[i])/log(branch_factor)) + 5\n",
    "                max_tree_depth = math.ceil(avg_tree_depth)\n",
    "\n",
    "            chart = self.parser.chart_parse((word for word, pos in corpus[i]))\n",
    "            possible_parses = len(chart.edges())\n",
    "\n",
    "            sent_stats['n_trees'] = n_trees\n",
    "            sent_stats['max_tree_depth'] = max_tree_depth\n",
    "            sent_stats['avg_tree_depth'] = avg_tree_depth\n",
    "            sent_stats['possible_parses'] = possible_parses\n",
    "            sent_stats['avg_noun_phrases'] = avg_noun_phrases\n",
    "            sent_stats['avg_prepositional_phrases'] = avg_noun_phrases\n",
    "            sent_stats['avg_sbars'] = avg_sbars\n",
    "            sent_stats['avg_nonterminals'] = avg_nonterminals\n",
    "            # etc.\n",
    "            print(sent_stats)\n",
    "            self.stats.append(sent_stats)\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"\n",
    "        Combines all the statistics together\n",
    "        \"\"\"\n",
    "        a_t = sum(s['n_trees'] for s in self.stats) / len(self.stats)\n",
    "        m_t_d = max(s['max_tree_depth'] for s in self.stats)\n",
    "        a_t_d = sum(s['avg_tree_depth'] for s in self.stats) / len(self.stats)\n",
    "        a_p_p = sum(s['possible_parses'] for s in self.stats) / len(self.stats)\n",
    "        a_n_p = sum(s['avg_noun_phrases'] for s in self.stats) / len(self.stats)\n",
    "        a_p_p = sum(s['avg_prepositional_phrases'] for s in self.stats) / len(self.stats)\n",
    "        a_s = sum(s['avg_sbars'] for s in self.stats) / len(self.stats)\n",
    "        a_n = sum(s['avg_nonterminals'] for s in self.stats) / len(self.stats)\n",
    "\n",
    "        return {\n",
    "            'avg_trees': a_t,\n",
    "            'max_tree_depth': m_t_d,\n",
    "            'avg_tree_depth': a_t_d,\n",
    "            'avg_possible_parses': a_p_p,\n",
    "            'avg_noun_phrases': a_n_p,\n",
    "            'avg_prepositional_phrases': a_p_p,\n",
    "            'avg_sbars': a_s,\n",
    "            'avg_nonterminals': a_n,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test stats class\n",
    "tagged_sentences = tag_sentences('The quick brown fox jumps over the lazy dog. ' +\n",
    "                                 'Now is the time for all good men to come to the aid of their party.')\n",
    "\n",
    "tree_stats = TreeStats(tagged_sentences)\n",
    "tree_stats.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test stats class\n",
    "tagged = tag_corpus(c5)\n",
    "tree_stats = TreeStats(tagged[1:6])\n",
    "tree_stats.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test globals\n",
    "productions = None\n",
    "init_productions()\n",
    "print(sum(len(p.rhs()) for p in productions), 'in', len(productions))\n",
    "print(branch_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all of these:\n",
    "tagged_sentences = tag_sentences('The quick brown fox jumps over the lazy dog. ' +\n",
    "                                 'Now is the time for all good men to come to the aid of their party.')\n",
    "print(create_terminals(tagged_sentences))\n",
    "\n",
    "t = get_trees(tagged_sentences[0])\n",
    "print('n_trees:', n_trees(t))\n",
    "print('max_tree_depth:', max_tree_depth(t))\n",
    "print('avg_tree_depth:', avg_tree_depth(t))\n",
    "print('*' * 80)\n",
    "t = get_trees(tagged_sentences[1])\n",
    "print('n_trees:', n_trees(t))\n",
    "print('max_tree_depth:', max_tree_depth(t))\n",
    "print('avg_tree_depth:', avg_tree_depth(t, tagged_sentences[1]))\n",
    "print('*' * 80)\n",
    "print('combined:', combined(tagged_sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse-tree-based Metrics\n",
    "\n",
    "These use the parse trees to measure things. We'll start with simple measurements, and build on top of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_productions(parse_tree, production):\n",
    "    \"\"\"Returns the number of productions of type `production` in the parse_tree\"\"\"\n",
    "    productions = list(parse_tree.subtrees(filter=lambda t: t.label() == production))\n",
    "    return len(productions)\n",
    "    \n",
    "def n_noun_phrases(parse_tree):\n",
    "    \"\"\"Return the number of noun phrases in the parse tree\"\"\"\n",
    "    return n_productions(parse_tree, 'NP')\n",
    "\n",
    "def n_prepositional_phrases(parse_tree):\n",
    "    \"\"\"Returns the number of prepositional phrases in the parse tree\"\"\"\n",
    "    return n_productions(parse_tree, 'PP')\n",
    "\n",
    "def n_nonterminals(parse_tree):\n",
    "    \"\"\"Find the number of nonterminal productions in a parse tree\"\"\"\n",
    "    return len(parse_tree.productions())\n",
    "\n",
    "def n_sbars(parse_tree):\n",
    "    \"\"\"Find the number of SBAR's in a parse tree\"\"\"\n",
    "    init_productions()\n",
    "    return n_productions(parse_tree, 'SBAR')\n",
    "\n",
    "def n_negations(sentence):\n",
    "    \"\"\"\n",
    "    Returns the number of negations in the sentence\n",
    "    \"\"\"\n",
    "    init_productions()\n",
    "    return 0  # TODO -- how!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test all of these:\n",
    "tagged_sentences = tag_sentences('The quick brown fox jumps over the lazy dog. ' +\n",
    "                                 'Now is the time for all good men to come to the aid of their party.')\n",
    "fox_trees = get_trees(tagged_sentence)\n",
    "\n",
    "print('n_productions:', n_productions(fox_trees[8], 'NP'))  # should be 9\n",
    "print('n_noun_phrases:', n_noun_phrases(fox_trees[8]))  # should be 9\n",
    "print('n_prepositional_phrases:', n_prepositional_phrases(fox_trees[8]))  # should be 1\n",
    "print('n_nonterminals:', n_nonterminals(fox_trees[8]))  # should be 1\n",
    "print('n_sbars:', n_sbars(fox_trees[800]))  # should be 1\n",
    "\n",
    "\n",
    "\n",
    "# print('max_tree_depth:', max_tree_depth(tagged_sentences[0]))\n",
    "# print('avg_tree_depth:', avg_tree_depth(tagged_sentences[0]))\n",
    "# print('*' * 80)\n",
    "# print('n_trees:', n_trees(tagged_sentences[1]))\n",
    "# print('max_tree_depth:', max_tree_depth(tagged_sentences[1]))\n",
    "# print('avg_tree_depth:', avg_tree_depth(tagged_sentences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = fox_trees[800]\n",
    "print(len(f.productions()))\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability\n",
    "\n",
    "In this part I:\n",
    "\n",
    "1. Calculate all of these stats for the pieces of the corpus\n",
    "\n",
    "2. Calculate the SMOG scores\n",
    "\n",
    "3. Figure out some relationship $f$ as above.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. parse an entire corpus at once. √\n",
    "# 2. get a parser for it, relatively optimized\n",
    "# 3. get the stats for it\n",
    "\n",
    "def tag_corpus(corpus):\n",
    "    \"\"\"\n",
    "    The corpus is composed of newline-separated paragraphs of sentences.\n",
    "    `tag_corpus` converts this into a list of POS-tagged sentences,\n",
    "    ignoring paragraphs.\n",
    "    \"\"\"\n",
    "    return tag_sentences(corpus)\n",
    "\n",
    "def combined_textual_stats(corpus):\n",
    "    \"\"\"\n",
    "    For a corpus (a list of POS-tagged sentences), determine all statistics as above\n",
    "    and return a dictionary of these.\n",
    "    \"\"\"\n",
    "    n_s = len(corpus)\n",
    "    n_w = sum((n_words(sentence) for sentence in corpus))\n",
    "    a_w_l = sum(sum(word_lengths(sentence)) for sentence in corpus) / n_w\n",
    "    s = sum( sum(syllables(sentence)) for sentence in corpus)\n",
    "    n_m_w = sum((n_monosyllable_words(sentence) for sentence in corpus))\n",
    "    n_p_w = sum((n_polysyllable_words(sentence) for sentence in corpus))\n",
    "    \n",
    "    pos = {pos for sentence in corpus for pos in POSs(sentence)}\n",
    "    n_pos = len(pos)\n",
    "    n_pro = sum(n_pronouns(sentence) for sentence in corpus)\n",
    "    n_r_p = sum(n_repeated_possessives(sentence) for sentence in corpus)\n",
    "    n_r_a = sum(n_repeated_adverbs(sentence) for sentence in corpus)\n",
    "\n",
    "    return {\n",
    "            'n_sents': n_s,\n",
    "            'n_words': n_w,\n",
    "            'avg_word_length': a_w_l,\n",
    "            'syllables': s,\n",
    "            'n_monosyllable_words': n_m_w,\n",
    "            'n_polysyllable_words': n_p_w,\n",
    "            'n_POSs': n_pos,\n",
    "            'n_pronouns': n_pro,\n",
    "            'n_repeated_possessives': n_r_p,\n",
    "            'n_repeated_adverbs': n_r_a\n",
    "           }\n",
    "\n",
    "def combined_tree_stats(corpus):\n",
    "    \"\"\"\n",
    "    For a corpus (a list of POS-tagged sentences), determine all statistics as above\n",
    "    and return a dictionary of these.\n",
    "    \"\"\"\n",
    "    n_s = len(corpus)\n",
    "\n",
    "    p_p = sum(possible_parses(sentence) for sentence in corpus)\n",
    "    n_t = sum(n_trees(sentence) for sentence in corpus)\n",
    "    m_t_d = max(max_tree_depth(sentence) for sentence in corpus)\n",
    "    a_t_d = sum(avg_tree_depth(sentence) for sentence in corpus) / n_s  # This isn't quite accurate, but it's okay\n",
    "    \n",
    "    return {\n",
    "        'possible_parses': p_p,\n",
    "        'n_trees': n_t,\n",
    "        'max_tree_depth': m_t_d,\n",
    "        'avg_tree_depth': a_t_d\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tree_depth(tagged[0])\n",
    "# I need to: 1. Not keep recreating a parser\n",
    "# 2. Reuse the trees I create already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Corpora/third-grade.txt')\n",
    "c3 = file.read()\n",
    "file.close()\n",
    "file = open('Corpora/fourth-grade.txt')\n",
    "c4 = file.read()\n",
    "file.close()\n",
    "file = open('Corpora/fifth-grade.txt')\n",
    "c5 = file.read()\n",
    "file.close()\n",
    "\n",
    "tagged = tag_corpus(c3)\n",
    "print(flesch_kincaid_grade_level(tagged))\n",
    "tagged = tag_corpus(c4)\n",
    "print(flesch_kincaid_grade_level(tagged))\n",
    "tagged = tag_corpus(c5)\n",
    "print(flesch_kincaid_grade_level(tagged))\n",
    "\n",
    "print(combined_textual_stats(tagged))\n",
    "\n",
    "print(combined_tree_stats(tagged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = (t for s in c5.split('\\n') for t in nltk.sent_tokenize(s))\n",
    "tagged = tag_corpus(c5)\n",
    "list(zip([n_repeated_adverbs(sentence) for sentence in tagged], tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
