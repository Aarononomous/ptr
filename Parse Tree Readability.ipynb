{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Tree Readability\n",
    "\n",
    "This Jupyter notebook is for developing, explaining, and experimenting with parse tree readability, a measure of readability I'm developing based on the premises that word length and sentence length are not the most important qualities of a piece of text when it comes to determining the difficulty of understanding it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Imports\n",
    "\n",
    "We'll need several libraries for the corpora, for simple text analysis, and for parsing sentences.\n",
    "\n",
    "In addition, we'll set up this notebook's graphing displays and create global variables for global things, such as American English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from math import sqrt\n",
    "import nltk\n",
    "from nltk import Tree\n",
    "import textacy\n",
    "import pyphen  # hyphenation library\n",
    "import spacy\n",
    "\n",
    "pyphen.language_fallback('en_US')\n",
    "dic = pyphen.Pyphen(lang='en_US')\n",
    "\n",
    "en_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we'll do is pull in our corpus. The corpus needs to contain a variety of reading levels within it.\n",
    "\n",
    "Additionally, we need to have an already-parse corpus. We can use the Penn Treebank for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj = nltk.corpus.treebank.parsed_sents('wsj_0012.mrg')\n",
    "# Let's look at a sample sentence\n",
    "w = wsj[0]\n",
    "print(w)  # the tree structure\n",
    "print(w.pos())  # a list of tagged words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production = w.productions()[0]\n",
    "print(production)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I'm able to, I'd like to use tagged, but non-parsed sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: get some small texts which are tagged with Penn Treebank tags\n",
    "\n",
    "\n",
    "So, what are the reading levels of these per the current gold standard SMOG?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: get the SMOG scores of each one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap of the SMOG scores.\n",
    "\n",
    "[[ TODO: if I'm going to not use individual works, but per-genre works, I'll need to be politic about leaping to conclusions. ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's begin to create our parsing functions. These are going to start simply, and get more complicated.\n",
    "\n",
    "## Textual metrics\n",
    "\n",
    "Let's begin with text-based metrics: sentence length and syllabification. We'll use the syllabification library `pyphen` to count the syllables in each word.\n",
    "\n",
    "We can build from these to create the standard text-based metrics, such as SMOG, Flesch-Kincaid, etc.\n",
    "\n",
    "Also note that every one of these takes a POS-tagged sentence, such as in the Treebank corpus, arranged as a list of (word/punctuation, POS) tuples. They do not take text, so it's important, if you want to use these on text, to tag the text beforehand. This can be done with, e.g., spaCy:\n",
    "\n",
    "```python\n",
    "doc = en_nlp(u'One morning, when Gregor Samsa woke from troubled dreams, \\\n",
    "he found himself transformed in his bed into a horrible vermin. He lay on \\\n",
    "his armour-like back, and if he lifted his head a little he could see his \\\n",
    "brown belly, slightly domed and divided by arches into stiff sections.')\n",
    "[[(word.text, word.tag_) for word in sent] for sent in doc.sents]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def depunctuate(tagged_sentence):\n",
    "    \"\"\"From a tagged sentence (as a list), returns a sentence without punctuation\"\"\"\n",
    "    punct_tags = set(['(', ')', ',', '--', '.', ':'])\n",
    "    return [(word, tag) for word, tag in tagged_sentence if tag not in punct_tags]\n",
    "    \n",
    "    \n",
    "def n_words(tagged_sentence):\n",
    "    \"\"\"\n",
    "    From a tagged sentence (as a list), returns the number of words in it,\n",
    "    sans punctuation.\n",
    "    \"\"\"\n",
    "    return len(depunctuate(tagged_sentence))\n",
    "\n",
    "def word_lengths(tagged_sentence):\n",
    "    \"\"\"\n",
    "    From a tagged sentence (as a list), returns a list of non-punctation\n",
    "    word lengths\n",
    "    \"\"\"\n",
    "    return [len(word) for word, pair in depunctuate(tagged_sentence)]\n",
    "\n",
    "def avg_word_length(tagged_sentence):\n",
    "    \"\"\"The average word length of the sentence\"\"\"\n",
    "    l = word_lengths(tagged_sentence)\n",
    "    return sum(l) / len(l)\n",
    "\n",
    "def syllables(tagged_sentence):\n",
    "    \"\"\"\n",
    "    From a tagged sentence (as a list), returns a list of non-punctation\n",
    "    syllables per word.\n",
    "    \"\"\"\n",
    "    return [len(dic.positions(word)) + 1 for word, pair in depunctuate(tagged_sentence)]\n",
    "\n",
    "def n_monosyllable_words(tagged_sentence):\n",
    "    \"\"\"Returns the number of one syllable words in the (word, tag)-list sentence\"\"\"\n",
    "    return len([sylls for sylls in syllables(tagged_sentence) if sylls == 1])\n",
    "\n",
    "def n_polysyllable_words(tagged_sentence):\n",
    "    \"\"\"Returns the number of 3+ syllable words in the (word, tag)-list sentence\"\"\"\n",
    "    return len([sylls for sylls in syllables(tagged_sentence) if sylls >= 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Readability Metrics\n",
    "\n",
    "With these building blocks in place, we can create our own versions of the standard readability metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SMOG(text):\n",
    "    \"\"\"\n",
    "    Computes the SMOG score of a piece of text, in this case, a list of tagged sentences.\n",
    "    There must be at least 30 sentences in the text or an Error will be thrown.\n",
    "    \n",
    "    McLaughlin, G. Harry (May 1969). \"SMOG Grading — a New Readability Formula\" (PDF).\n",
    "    Journal of Reading. 12 (8): 639–646.\n",
    "    \"\"\"\n",
    "    if len(text) < 30:\n",
    "        raise ValueError('There must be at least 30 sentences in the input for an accurate \\\n",
    "        readability score.')\n",
    "    \n",
    "    n_polysyllables = sum(n_polysyllable_words(sentence) for sentence in text)\n",
    "    grade = 1.0430 * sqrt(n_polysyllables * (30 / len(text))) + 3.1291\n",
    "    return grade\n",
    "    \n",
    "def flesch_kincaid_grade_level(text):\n",
    "    \"\"\"\n",
    "    Computes the Flesch-Kincaid grade level of a piece of text, in this case,\n",
    "    a list of tagged sentences.\n",
    "\n",
    "    Kincaid JP, Fishburne RP Jr, Rogers RL, Chissom BS (February 1975).\n",
    "    \"Derivation of new readability formulas (Automated Readability Index,\n",
    "    Fog Count and Flesch Reading Ease Formula) for Navy enlisted personnel\".\n",
    "    Research Branch Report 8-75, Millington, TN: Naval Technical Training,\n",
    "    U. S. Naval Air Station, Memphis, TN.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_words = sum(n_words(sentence) for sentence in text)\n",
    "    total_syllables = sum(sum(syllables(sentence)) for sentence in text)\n",
    "    grade = 0.39 * (total_words / len(text)) + 11.8 * (total_syllables/total_words) - 15.59\n",
    "    return grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: test these against the built-in versions in spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next round of text analysis functions, let's dig a little deeper into the POS analysis. Remember that this is still sans parse-tree-ification.\n",
    "\n",
    "## Tag-based Metrics\n",
    "\n",
    "These use the POS tags. Since we're using the Penn Treebank, we need to make sure that whatever tags we're using are within this tagset; some tagged texts from the Brown Corpus, for example, use other tagsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_POSs(tagged_sentence):\n",
    "    \"\"\"Returns the number of unique POS's in the tagged sentence\"\"\"\n",
    "    return len(set(pos for word, pos in tagged_sentence))\n",
    "\n",
    "# TODO: what else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based Metrics\n",
    "\n",
    "These use the structure of the parse trees, not the productions (yet!) to measure things. We'll start with simple measurements, and build on top of those.\n",
    "\n",
    "First, of course, we need to get the parse trees. [ TODO ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_trees(sentence):\n",
    "    \"\"\"\n",
    "    Returns all possible parse trees from a sentence.\n",
    "    The sentence is POS-tagged, and this returns a list of nltk.Trees\n",
    "    \"\"\"\n",
    "    pass  # TODO\n",
    "\n",
    "def tree_depth(tree):\n",
    "    \"\"\"Returns the tree depth of an individual parse tree. O(n)\"\"\"\n",
    "    return 0  # TODO - there may be a built-in method for this\n",
    "\n",
    "def max_tree_depth(trees):\n",
    "    \"\"\"Returns the max tree depth over a collection of parse trees\"\"\"\n",
    "    return max((tree_depth(tree) for tree in trees))\n",
    "\n",
    "def avg_tree_depth(trees):\n",
    "    \"\"\"Returns the average tree depth over a collection of parse trees\"\"\"\n",
    "    return sum((tree_depth(tree) for tree in trees))/ len(trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse-tree-based Metrics\n",
    "\n",
    "These use the parse trees to measure things. We'll start with simple measurements, and build on top of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_productions(parse_tree, production):\n",
    "    \"\"\"Returns the number of productions of type `production` in the parse_tree\"\"\"\n",
    "    return 0  # TODO\n",
    "    \n",
    "def n_noun_phrases(parse_tree):\n",
    "    \"\"\"Returns the number of noun phrases in the parse tree\"\"\"\n",
    "    return n_productions(parse_tree, 'NP')\n",
    "\n",
    "def n_subordinate_clauses(parse_tree):\n",
    "    \"\"\"Returns the number of subordinate clauses in the parse tree\"\"\"\n",
    "    return 0  # TODO\n",
    "\n",
    "def possible_pos_tags(sentence):\n",
    "    \"\"\"\n",
    "    Words may be parsed as multiple parts of speech; for example, \"parts\" can\n",
    "    be a plural noun or a present singular verb.\n",
    "    Takes a tagged sentence, ignoring the current tags, and maps the list to\n",
    "    a list of the count of POS's available for each word.\n",
    "    E.g.: [(\"Parts\", 'NNS'), ('of', 'IN')...] -> [2, 1, ...]\n",
    "    \"\"\"\n",
    "    return [1 for word_pair in sentence]  # TODO\n",
    "\n",
    "def possible_taggings(sentence):\n",
    "    \"\"\"\n",
    "    Similar to the above, but sees how many of these possible POS's were\n",
    "    _actually_ considered by the CKY algorithm (or whatever the library I'm\n",
    "    using to tag these uses).\n",
    "    \"\"\"\n",
    "    return [1 for word_pair in sentence]  # TODO\n",
    "    # I think I can't use spaCy for this; I'll look into CoreNLP or SyntaxNet or nltk\n",
    "\n",
    "def n_negations(sentence):\n",
    "    \"\"\"\n",
    "    Returns the number of negations in the sentence\n",
    "    \"\"\"\n",
    "    return 0  # TODO -- how!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability\n",
    "\n",
    "In this part I:\n",
    "\n",
    "1. Calculate all of these stats for the pieces of the corpus\n",
    "\n",
    "2. Calculate the SMOG scores\n",
    "\n",
    "3. Figure out some relationship $f$ as above.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combined_textual_stats(corpus):\n",
    "    \"\"\"\n",
    "    For a corpus (a list of POS-tagged sentences), determine all statistics as above\n",
    "    and return a dictionary of these.\n",
    "    \"\"\"\n",
    "    n_w = sum((n_words(sentence) for sentence in corpus))\n",
    "    a_w_l = sum(sum(word_lengths(sentence)) for sentence in corpus) / n_w\n",
    "    s = sum( sum(syllables(sentence)) for sentence in corpus)\n",
    "    n_m_w = sum((n_monosyllable_words(sentence) for sentence in corpus))\n",
    "    n_p_w = sum((n_polysyllable_words(sentence) for sentence in corpus))\n",
    "    \n",
    "    return {\n",
    "            'n_words': n_w,\n",
    "            'avg_word_length': a_w_l,\n",
    "            'syllables': s,\n",
    "            'n_monosyllable_words': n_m_w,\n",
    "            'n_polysyllable_words': n_p_w,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
